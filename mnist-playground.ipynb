{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-15T16:22:47.365023Z","iopub.status.busy":"2024-07-15T16:22:47.364626Z","iopub.status.idle":"2024-07-15T16:22:49.349077Z","shell.execute_reply":"2024-07-15T16:22:49.347576Z","shell.execute_reply.started":"2024-07-15T16:22:47.364975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of training data: (60000, 785)\n","Shape of test_data: (10000, 785)\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import numpy.typing as npt\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import skimage as ski\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","        \n","\n","datapath_train = \"./mnist_data/mnist_train.csv\"\n","datapath_test = \"./mnist_data/mnist_test.csv\"\n","\n","datatype = np.float64\n","\n","train_data = np.loadtxt(datapath_train, dtype=datatype, delimiter=\",\", skiprows=1)\n","test_data = np.loadtxt(datapath_test, dtype=datatype, delimiter=\",\", skiprows=1)\n","print(f\"Shape of training data: {train_data.shape}\")\n","print(f\"Shape of test_data: {test_data.shape}\")\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of train labels: (60000,)\n","Shape of train values: (60000, 784)\n","Shape of test labels: (10000,)\n","Shape of test values: (10000, 784)\n"]}],"source":["train_labels: npt.NDArray = train_data[:, 0]\n","print(f\"Shape of train labels: {train_labels.shape}\")\n","train_vals = train_data[:, 1:] / 255\n","print(f\"Shape of train values: {train_vals.shape}\")\n","\n","\n","test_labels: npt.NDArray = test_data[:, 0]\n","print(f\"Shape of test labels: {test_labels.shape}\")\n","test_vals = test_data[:, 1:] / 255\n","print(f\"Shape of test values: {test_vals.shape}\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def sigmoid(x: npt.NDArray) -> npt.NDArray:\n","    return 1 / (1 + np.exp(-x))\n","\n","def mse_loss(predictions: npt.NDArray, labels: npt.NDArray):\n","    truth_table = np.identity(10, dtype=datatype)\n","    y_true = truth_table[labels.astype(dtype=np.uint8)]\n","    n = len(y_true)\n","    sum_each_column = np.sum(((predictions - y_true) ** 2) / n, axis=1)\n","    return sum_each_column\n","\n","def ReLU(x: npt.NDArray) -> npt.NDArray:\n","    \"\"\"ReLU(x) = max(0, x)\"\"\"\n","    return x * (x > 0)\n","\n","def percentages(x: npt.NDArray) -> npt.NDArray:\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","def random_weights(dimensions: npt.ArrayLike, low: float = -1, high: float = 1):\n","    range = high - low\n","    return np.random.rand(*dimensions) * range + low\n","\n","def print_percentages(percentages_array: npt.NDArray) -> None:\n","    for i, p in enumerate(percentages_array):\n","        print(f\"{i} = {round(p * 100, 3)}%\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T16:35:15.653098Z","iopub.status.busy":"2024-07-15T16:35:15.652673Z","iopub.status.idle":"2024-07-15T16:35:15.823198Z","shell.execute_reply":"2024-07-15T16:35:15.818978Z","shell.execute_reply.started":"2024-07-15T16:35:15.653063Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Truth: 5.0\n","0 = 14.597%\n","1 = 14.597%\n","2 = 5.4%\n","3 = 14.597%\n","4 = 5.403%\n","5 = 5.37%\n","6 = 14.597%\n","7 = 5.47%\n","8 = 14.597%\n","9 = 5.37%\n"]}],"source":["num_mid_layer = 1000\n","hidden_layer_size = 100\n","\n","# 784 -> `hidden_layer_size` -> 10\n","\n","hidden_layer_weights = random_weights((hidden_layer_size, 784)) # Random values in the range [-1, 1)\n","hidden_layer_bias = random_weights([hidden_layer_size])\n","\n","output_layer_weights = random_weights((10, hidden_layer_size))\n","output_layer_bias = random_weights([10])\n","\n","hidden_layer_activations = ReLU((hidden_layer_weights @ train_vals[0].T) + hidden_layer_bias)\n","output_layer_activations = sigmoid((output_layer_weights @ hidden_layer_activations.T) + output_layer_bias)\n","\n","print(f\"Truth: {train_labels[0]}\")\n","print_percentages(percentages(output_layer_activations))\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-15T16:36:16.960846Z","iopub.status.busy":"2024-07-15T16:36:16.960458Z","iopub.status.idle":"2024-07-15T16:36:16.968917Z","shell.execute_reply":"2024-07-15T16:36:16.967874Z","shell.execute_reply.started":"2024-07-15T16:36:16.960817Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'predictions' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(mse_loss(\u001b[43mpredictions\u001b[49m, train_labels))\n","\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"]}],"source":["np.sum(mse_loss(predictions, train_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0 loss = 4.581399954494977\n","Iteration 1 loss = 4.613280041924473\n","New model found! New loss: 3.9159558033012525\n","Iteration 3 loss = 4.1407764306675405\n","Iteration 4 loss = 4.544472578421632\n","Iteration 5 loss = 5.263302532772334\n","New model found! New loss: 3.8935145021478643\n","Iteration 7 loss = 4.847840031700671\n","Iteration 8 loss = 4.145850498418171\n","Iteration 9 loss = 4.184187073433147\n","Iteration 10 loss = 4.084961245252244\n","New model found! New loss: 3.2380094638650325\n","Iteration 12 loss = 4.87713218892259\n","Iteration 13 loss = 4.599684967210228\n","Iteration 14 loss = 3.5842888841318854\n","Iteration 15 loss = 4.409398341874957\n","Iteration 16 loss = 3.5176616441756465\n","Iteration 17 loss = 3.4149532938473492\n","Iteration 18 loss = 5.106767460656234\n","Iteration 19 loss = 5.115765641041121\n","Iteration 20 loss = 4.723975205763852\n","Iteration 21 loss = 3.718606878629781\n","Iteration 22 loss = 3.6759505342788246\n","Iteration 23 loss = 4.0186701540349015\n","Iteration 24 loss = 4.274554085918562\n","Iteration 25 loss = 4.364622190234946\n","Iteration 26 loss = 4.123150555839782\n","Iteration 27 loss = 5.025830478171114\n","Iteration 28 loss = 3.297010691132074\n","Iteration 29 loss = 5.4709330737698485\n","Iteration 30 loss = 4.286274474075997\n","Iteration 31 loss = 4.25871190547517\n","Iteration 32 loss = 3.990081919189075\n","Iteration 33 loss = 4.399174511759724\n","Iteration 34 loss = 3.8095066643742492\n","Iteration 35 loss = 3.8228421582548613\n","Iteration 36 loss = 4.586228060366404\n","Iteration 37 loss = 3.4362988210776315\n","Iteration 38 loss = 4.069147517404263\n","Iteration 39 loss = 4.073391506512515\n","Iteration 40 loss = 4.603154849443098\n","Iteration 41 loss = 3.4940734338262893\n","Iteration 42 loss = 3.9244764882989074\n","Iteration 43 loss = 4.920329592353903\n","Iteration 44 loss = 4.598012207611372\n","Iteration 45 loss = 4.066679911104905\n","Iteration 46 loss = 4.123048792120794\n","Iteration 47 loss = 4.675499810879794\n","Iteration 48 loss = 4.361248192807035\n","Iteration 49 loss = 4.465387998761755\n","Iteration 50 loss = 3.929871625118855\n","Iteration 51 loss = 5.286346371602159\n","Iteration 52 loss = 4.438970679791135\n","Iteration 53 loss = 4.147442801493981\n","Iteration 54 loss = 3.9992430308333033\n","Iteration 55 loss = 3.269150086914297\n","Iteration 56 loss = 3.776553965414517\n","Iteration 57 loss = 4.740451096568396\n","Iteration 58 loss = 3.9470900438045153\n","Iteration 59 loss = 3.702734863319368\n","Iteration 60 loss = 4.1660550179364115\n","Iteration 61 loss = 3.886454756896047\n","Iteration 62 loss = 4.044791253233824\n","Iteration 63 loss = 5.141361823060205\n","Iteration 64 loss = 3.6467517323400473\n","Iteration 65 loss = 4.785871898631631\n","Iteration 66 loss = 3.8592298696211818\n","Iteration 67 loss = 4.848772041598439\n","Iteration 68 loss = 4.551529486269298\n","Iteration 69 loss = 3.269232541180776\n","Iteration 70 loss = 3.8270964283458677\n","Iteration 71 loss = 3.688161175768978\n","Iteration 72 loss = 4.788541195762157\n","Iteration 73 loss = 4.107392922739416\n","Iteration 74 loss = 4.6570027464888835\n","Iteration 75 loss = 3.775915134420254\n","Iteration 76 loss = 5.427729289883615\n","Iteration 77 loss = 3.792606914920434\n","Iteration 78 loss = 5.231435001315509\n","Iteration 79 loss = 4.406876394724694\n","New model found! New loss: 3.215200673349911\n","Iteration 81 loss = 4.907113947602128\n","Iteration 82 loss = 4.820441743600425\n","Iteration 83 loss = 4.484374879650878\n","Iteration 84 loss = 3.9496915617461497\n","Iteration 85 loss = 4.966167722267071\n","Iteration 86 loss = 4.367670576856006\n","Iteration 87 loss = 4.084176484535918\n","Iteration 88 loss = 5.913511766787371\n","Iteration 89 loss = 3.6103460544381383\n","Iteration 90 loss = 5.037974892084112\n","Iteration 91 loss = 4.480117930061494\n","Iteration 92 loss = 4.183750959085101\n","Iteration 93 loss = 3.784273934958293\n","Iteration 94 loss = 4.525213062148486\n","Iteration 95 loss = 4.0785642161303794\n","Iteration 96 loss = 4.507664377086678\n","Iteration 97 loss = 4.653468489126674\n","Iteration 98 loss = 3.6200055180081927\n","Iteration 99 loss = 3.5313157663957973\n","Iteration 100 loss = 4.444194226819854\n","Iteration 101 loss = 4.002573687352414\n","Iteration 102 loss = 3.3951881593253277\n","Iteration 103 loss = 4.187668246717726\n","Iteration 104 loss = 4.2368512225980695\n","Iteration 105 loss = 3.675479128748904\n","Iteration 106 loss = 3.480364126203846\n","Iteration 107 loss = 4.499584419367025\n","Iteration 108 loss = 3.5743729189563784\n","Iteration 109 loss = 4.943495446975693\n","Iteration 110 loss = 4.117206190948505\n","Iteration 111 loss = 6.056183922949512\n","Iteration 112 loss = 3.549426148289541\n","Iteration 113 loss = 4.694272211728795\n","Iteration 114 loss = 4.241359281522199\n","Iteration 115 loss = 4.176214478383436\n","Iteration 116 loss = 4.135129392185286\n","Iteration 117 loss = 4.7640820046547905\n","Iteration 118 loss = 4.002463910381377\n","Iteration 119 loss = 4.61256856435801\n","Iteration 120 loss = 3.9257799119305803\n","Iteration 121 loss = 4.766002633179216\n","Iteration 122 loss = 4.230158761833248\n","Iteration 123 loss = 3.927317476740561\n","Iteration 124 loss = 4.200205761699984\n","Iteration 125 loss = 4.182501112886554\n","Iteration 126 loss = 3.2841693807846677\n","Iteration 127 loss = 4.1947439832073705\n","Iteration 128 loss = 4.114555651351176\n","Iteration 129 loss = 4.733342061481108\n","Iteration 130 loss = 3.5251410323809758\n","Iteration 131 loss = 4.639588252692595\n","Iteration 132 loss = 4.457700604689612\n","Iteration 133 loss = 4.074694508324489\n","Iteration 134 loss = 5.422741203750406\n","Iteration 135 loss = 5.066470785877686\n","Iteration 136 loss = 3.8154049351775976\n","Iteration 137 loss = 5.001392743305421\n","Iteration 138 loss = 4.449901258239918\n","Iteration 139 loss = 4.486141674206399\n","Iteration 140 loss = 3.703203091969087\n","Iteration 141 loss = 3.961033360137598\n","Iteration 142 loss = 4.748031012187232\n","Iteration 143 loss = 4.598850326898835\n","Iteration 144 loss = 4.467744842838216\n","Iteration 145 loss = 4.421973668824269\n","Iteration 146 loss = 4.822652577853657\n","Iteration 147 loss = 4.501737676169151\n","Iteration 148 loss = 4.24956062967752\n","Iteration 149 loss = 3.7748846875006272\n","Iteration 150 loss = 4.507063967864444\n","Iteration 151 loss = 4.825969150904372\n","Iteration 152 loss = 3.8549508552121594\n","Iteration 153 loss = 4.862466980499458\n","Iteration 154 loss = 3.61328518873449\n","Iteration 155 loss = 4.747569045023521\n","New model found! New loss: 3.1270720655930897\n","Iteration 157 loss = 3.48406723351279\n","Iteration 158 loss = 4.058740266128854\n","Iteration 159 loss = 3.4110473247288766\n","Iteration 160 loss = 4.883888610668466\n","Iteration 161 loss = 4.722535166964211\n","Iteration 162 loss = 4.330651765951697\n","Iteration 163 loss = 4.409532974747053\n","Iteration 164 loss = 5.019367101490068\n","Iteration 165 loss = 4.852976324114769\n","Iteration 166 loss = 4.031754803037972\n","Iteration 167 loss = 3.9317396517755103\n","Iteration 168 loss = 4.391993279466333\n","Iteration 169 loss = 4.378579895900199\n","Iteration 170 loss = 4.363801134083111\n","Iteration 171 loss = 4.714427404622743\n","Iteration 172 loss = 4.690256550276755\n","Iteration 173 loss = 3.798609366851157\n","Iteration 174 loss = 4.486877234270337\n","Iteration 175 loss = 4.187270256372738\n","Iteration 176 loss = 4.527234030008443\n","Iteration 177 loss = 4.074540305816541\n","Iteration 178 loss = 4.6188707992602325\n","Iteration 179 loss = 3.924818499701816\n","New model found! New loss: 3.0494626326745315\n","Iteration 181 loss = 4.640655740971922\n","Iteration 182 loss = 4.2226584739478605\n","Iteration 183 loss = 3.42613249291243\n","Iteration 184 loss = 5.1379513367453935\n","Iteration 185 loss = 3.8043732988230263\n","Iteration 186 loss = 3.3704201855259743\n","Iteration 187 loss = 4.877216065815365\n","Iteration 188 loss = 4.547276636034799\n","Iteration 189 loss = 4.289143033779461\n","Iteration 190 loss = 3.349803456554933\n","Iteration 191 loss = 5.113061849852252\n","Iteration 192 loss = 4.480533247159889\n","Iteration 193 loss = 4.010636107628345\n","Iteration 194 loss = 5.385798618276154\n","Iteration 195 loss = 3.369435384852526\n","Iteration 196 loss = 4.357262033642805\n","Iteration 197 loss = 5.010301513905263\n","Iteration 198 loss = 4.257235663838613\n","Iteration 199 loss = 4.816792906025797\n","Iteration 200 loss = 4.1964002508199165\n","Iteration 201 loss = 4.605019005703407\n","Iteration 202 loss = 4.430896764485055\n","Iteration 203 loss = 3.868265740263665\n","Iteration 204 loss = 3.993334408056876\n","Iteration 205 loss = 4.277810448162274\n","Iteration 206 loss = 4.655571197510104\n","Iteration 207 loss = 4.227489267843532\n","Iteration 208 loss = 4.629511779312444\n","Iteration 209 loss = 4.072705330908331\n","Iteration 210 loss = 3.8104115117615143\n","Iteration 211 loss = 3.8529820288135106\n","Iteration 212 loss = 3.9537415511641907\n","Iteration 213 loss = 4.00331862675296\n","Iteration 214 loss = 4.025035605088964\n","Iteration 215 loss = 4.119589327834164\n","Iteration 216 loss = 3.83361077936642\n","Iteration 217 loss = 3.9854979308887484\n","Iteration 218 loss = 4.9131272238870505\n","Iteration 219 loss = 4.4077883469842885\n","Iteration 220 loss = 4.426569922557891\n","Iteration 221 loss = 4.406300026930798\n","Iteration 222 loss = 5.383167247332521\n","Iteration 223 loss = 4.518762430126412\n","Iteration 224 loss = 4.928910807533705\n","Iteration 225 loss = 3.894818035488245\n","New model found! New loss: 2.8007996987802053\n","Iteration 227 loss = 4.006414822659059\n","Iteration 228 loss = 4.9936163375515115\n","Iteration 229 loss = 4.001178316563376\n","Iteration 230 loss = 4.348180141950789\n","Iteration 231 loss = 4.366234369836353\n","Iteration 232 loss = 4.425884135105219\n","Iteration 233 loss = 3.7250010856976816\n","Iteration 234 loss = 5.006647668453643\n","Iteration 235 loss = 5.551009530689171\n","Iteration 236 loss = 4.264572003530331\n","Iteration 237 loss = 5.177230166314954\n","Iteration 238 loss = 5.305991568405251\n","Iteration 239 loss = 4.667945614109071\n","Iteration 240 loss = 5.090636507710498\n","Iteration 241 loss = 3.7902620487496725\n","Iteration 242 loss = 4.553030260880682\n","Iteration 243 loss = 3.92900966607779\n","Iteration 244 loss = 3.821209536637863\n","Iteration 245 loss = 4.307922980079217\n","Iteration 246 loss = 3.690859300069151\n","Iteration 247 loss = 3.452763628004449\n","New model found! New loss: 2.7030229520867373\n","Iteration 249 loss = 4.553129080561398\n","Iteration 250 loss = 3.3887009350446755\n","Iteration 251 loss = 4.141574580114326\n","Iteration 252 loss = 4.8117932609011405\n","Iteration 253 loss = 3.8172838828034896\n","Iteration 254 loss = 3.54683093098407\n","Iteration 255 loss = 3.8976350410213625\n","Iteration 256 loss = 4.157963236935839\n","Iteration 257 loss = 4.380944817093284\n","Iteration 258 loss = 4.375211081191833\n","Iteration 259 loss = 5.1538715617026485\n","Iteration 260 loss = 5.292596479458655\n","Iteration 261 loss = 3.500053729435557\n","Iteration 262 loss = 4.077836017960359\n","Iteration 263 loss = 4.088837178293964\n","Iteration 264 loss = 4.434831616263075\n","Iteration 265 loss = 4.359117282574008\n","Iteration 266 loss = 4.081847246279824\n","Iteration 267 loss = 4.1468885408835305\n","Iteration 268 loss = 4.151173247864005\n","Iteration 269 loss = 4.679623845182066\n","Iteration 270 loss = 4.823320923535067\n","Iteration 271 loss = 4.641117667929384\n","Iteration 272 loss = 4.259412864803882\n","Iteration 273 loss = 3.2369969709913557\n","Iteration 274 loss = 4.71896804171394\n","Iteration 275 loss = 4.264623438251428\n","Iteration 276 loss = 4.475548044703219\n","Iteration 277 loss = 6.197427834673659\n","Iteration 278 loss = 5.599690008072386\n","Iteration 279 loss = 4.477874426803283\n","Iteration 280 loss = 3.9512823997361037\n","Iteration 281 loss = 4.679171169378676\n","Iteration 282 loss = 5.387949749101009\n","Iteration 283 loss = 3.5203845045006514\n","Iteration 284 loss = 4.025711721516413\n","Iteration 285 loss = 3.805982477118279\n","Iteration 286 loss = 4.283506169517728\n","Iteration 287 loss = 4.975149814194407\n","Iteration 288 loss = 4.034110759122328\n","Iteration 289 loss = 5.298183527758249\n","Iteration 290 loss = 4.1974118101473765\n","Iteration 291 loss = 4.494121890734973\n","Iteration 292 loss = 4.308727206418712\n","Iteration 293 loss = 5.272979937209055\n","Iteration 294 loss = 3.4618442106143847\n","Iteration 295 loss = 3.2084648005548178\n","Iteration 296 loss = 3.7901210513866896\n","Iteration 297 loss = 4.081775795192132\n","Iteration 298 loss = 4.745762693251813\n","Iteration 299 loss = 5.26903605483456\n","Iteration 300 loss = 4.533917846684065\n","Iteration 301 loss = 4.96672383186879\n","Iteration 302 loss = 4.7061581138312105\n","Iteration 303 loss = 4.009046411077044\n","Iteration 304 loss = 2.910225079749155\n","Iteration 305 loss = 4.813208697030836\n","Iteration 306 loss = 3.572219921930666\n","Iteration 307 loss = 3.091499352913997\n","Iteration 308 loss = 4.454995549024913\n","Iteration 309 loss = 4.014214981046106\n","Iteration 310 loss = 4.601235517942638\n","Iteration 311 loss = 3.1063926493397087\n","Iteration 312 loss = 3.5950645808345736\n","Iteration 313 loss = 4.286284024285095\n","Iteration 314 loss = 3.3563182194282404\n","Iteration 315 loss = 4.014986707657539\n","Iteration 316 loss = 5.490274015645029\n","Iteration 317 loss = 4.122276606744624\n","Iteration 318 loss = 4.438808275755007\n","Iteration 319 loss = 3.829404818227264\n","Iteration 320 loss = 3.5569451673260737\n","Iteration 321 loss = 3.1756180136507086\n","Iteration 322 loss = 3.388540743421569\n","Iteration 323 loss = 3.644320564488627\n","Iteration 324 loss = 4.500692487434958\n","Iteration 325 loss = 4.077760478002042\n","Iteration 326 loss = 2.83133432920855\n","Iteration 327 loss = 3.8918707125875365\n","Iteration 328 loss = 4.275856822308028\n","Iteration 329 loss = 4.210737884784595\n","Iteration 330 loss = 4.389018592134627\n","Iteration 331 loss = 4.325593526143866\n","Iteration 332 loss = 5.05785264774574\n","Iteration 333 loss = 4.965213631593375\n","Iteration 334 loss = 4.168457754620079\n","Iteration 335 loss = 4.060108536126147\n","Iteration 336 loss = 3.6254430132942583\n","Iteration 337 loss = 4.608316571635699\n","Iteration 338 loss = 4.46777662587415\n","Iteration 339 loss = 4.003357831261161\n","Iteration 340 loss = 4.369311902396065\n","Iteration 341 loss = 4.581520202355778\n","Iteration 342 loss = 5.229957210715124\n","Iteration 343 loss = 5.373820849163145\n","Iteration 344 loss = 4.4173540099359805\n","Iteration 345 loss = 4.1427036342143415\n","Iteration 346 loss = 4.158424770038783\n","Iteration 347 loss = 4.762109270849655\n","Iteration 348 loss = 3.054418725704356\n","Iteration 349 loss = 5.030347957553033\n","Iteration 350 loss = 4.725143202181389\n","Iteration 351 loss = 4.484482485742458\n","Iteration 352 loss = 5.405741751608428\n","Iteration 353 loss = 4.30346084342935\n","Iteration 354 loss = 3.3184259763219672\n","Iteration 355 loss = 4.4322431559008475\n","Iteration 356 loss = 3.9586915866361987\n","Iteration 357 loss = 4.553340212806207\n","Iteration 358 loss = 3.8374791252471576\n","Iteration 359 loss = 4.6018746940224045\n","Iteration 360 loss = 4.3574860974936405\n","Iteration 361 loss = 4.946295744764694\n","Iteration 362 loss = 3.600597879027543\n","Iteration 363 loss = 3.7629347930596198\n","Iteration 364 loss = 3.456330437146334\n","Iteration 365 loss = 4.0778589843649815\n","Iteration 366 loss = 4.223802262007902\n","Iteration 367 loss = 5.044960250485146\n","Iteration 368 loss = 4.889752394493429\n","Iteration 369 loss = 4.723667266650433\n","Iteration 370 loss = 5.302581990032168\n","Iteration 371 loss = 3.3107418238010924\n","Iteration 372 loss = 4.644788948277221\n","Iteration 373 loss = 3.2111702057445592\n","Iteration 374 loss = 3.835199773146708\n","Iteration 375 loss = 4.500422548675886\n","Iteration 376 loss = 5.251145406585621\n","Iteration 377 loss = 3.671951733924514\n","Iteration 378 loss = 4.813445957777409\n","Iteration 379 loss = 4.9851018263494895\n","Iteration 380 loss = 5.311586729350719\n","Iteration 381 loss = 4.371280305064351\n","Iteration 382 loss = 2.9776920717984656\n","Iteration 383 loss = 4.569046743779953\n","Iteration 384 loss = 4.105848988723274\n","Iteration 385 loss = 4.025152431985432\n","Iteration 386 loss = 4.759263414766601\n","Iteration 387 loss = 4.109024494433431\n","Iteration 388 loss = 4.776773255607512\n","Iteration 389 loss = 3.623806567325215\n","Iteration 390 loss = 4.610802009134464\n","Iteration 391 loss = 2.9879424406124966\n","Iteration 392 loss = 4.381396801864706\n","Iteration 393 loss = 3.240769530993242\n","Iteration 394 loss = 4.568042220060862\n","Iteration 395 loss = 4.5794765574797465\n","Iteration 396 loss = 4.150236785540229\n","Iteration 397 loss = 3.9555325882098256\n","Iteration 398 loss = 3.7283067107423364\n","Iteration 399 loss = 3.600491190906232\n","Iteration 400 loss = 4.598123382349947\n","Iteration 401 loss = 4.542014498999714\n","Iteration 402 loss = 5.114394174477158\n","Iteration 403 loss = 3.8941634539576495\n","Iteration 404 loss = 3.2285259553921604\n","Iteration 405 loss = 4.081626918311257\n","Iteration 406 loss = 2.804052255611997\n","Iteration 407 loss = 4.280625796982694\n","Iteration 408 loss = 5.017327762647437\n","Iteration 409 loss = 4.358838387674843\n","Iteration 410 loss = 3.5488342627283274\n","Iteration 411 loss = 4.375761830433369\n","Iteration 412 loss = 4.0634216776084315\n","Iteration 413 loss = 4.400011005115934\n","Iteration 414 loss = 4.567057676810871\n","Iteration 415 loss = 3.6828034482785554\n","Iteration 416 loss = 3.7538028650578346\n","Iteration 417 loss = 5.074406606742269\n","Iteration 418 loss = 4.048656686181466\n","Iteration 419 loss = 4.246001422827749\n","Iteration 420 loss = 4.4760654784816705\n","Iteration 421 loss = 3.792515634866001\n","Iteration 422 loss = 4.1427786093155605\n","Iteration 423 loss = 4.195845289853394\n","Iteration 424 loss = 5.278041340003873\n","Iteration 425 loss = 5.324900500660126\n","Iteration 426 loss = 4.028050282803244\n","Iteration 427 loss = 4.412497983080389\n","Iteration 428 loss = 5.382321216251917\n","Iteration 429 loss = 3.8268176738193005\n","Iteration 430 loss = 3.3395013871824046\n","Iteration 431 loss = 2.9694512535023536\n","Iteration 432 loss = 5.490366963089913\n","Iteration 433 loss = 4.814287524408223\n","Iteration 434 loss = 4.39081385943629\n","Iteration 435 loss = 2.8012217695480657\n","Iteration 436 loss = 4.660215964026725\n","Iteration 437 loss = 3.6717667016426145\n","Iteration 438 loss = 4.157379510171269\n","Iteration 439 loss = 3.5213720398311192\n","Iteration 440 loss = 5.630644472911327\n","Iteration 441 loss = 4.950491123427148\n","Iteration 442 loss = 4.369441212734601\n","Iteration 443 loss = 4.303392811836695\n","Iteration 444 loss = 4.559151296450253\n","Iteration 445 loss = 4.887758976005681\n","Iteration 446 loss = 5.130936894228626\n","Iteration 447 loss = 3.5911276511171244\n","Iteration 448 loss = 5.234143692165637\n","Iteration 449 loss = 4.561677111833636\n","Iteration 450 loss = 3.7946356668237082\n","Iteration 451 loss = 3.64395823402428\n","Iteration 452 loss = 3.6013013161113947\n","Iteration 453 loss = 4.47083310955762\n","Iteration 454 loss = 3.852687015072473\n","Iteration 455 loss = 3.287584780562361\n","Iteration 456 loss = 3.1845083570875503\n","Iteration 457 loss = 4.7121465799700974\n","Iteration 458 loss = 3.7518372099048167\n","Iteration 459 loss = 5.330536265366171\n","Iteration 460 loss = 4.531802194932151\n","Iteration 461 loss = 2.7385131746319837\n","Iteration 462 loss = 4.640017840495565\n","Iteration 463 loss = 3.8185661364938133\n","Iteration 464 loss = 3.8333739686123565\n","Iteration 465 loss = 5.235733483207871\n","Iteration 466 loss = 4.0778611734374905\n","Iteration 467 loss = 3.8820294409799745\n","Iteration 468 loss = 4.277820996184865\n","Iteration 469 loss = 5.4100620402741235\n","Iteration 470 loss = 2.736734794073916\n","Iteration 471 loss = 4.656155815918119\n","Iteration 472 loss = 3.880821414426155\n","Iteration 473 loss = 4.5036987342976715\n","Iteration 474 loss = 4.591095308470271\n","Iteration 475 loss = 4.6684465205658565\n","Iteration 476 loss = 4.437799022353813\n","Iteration 477 loss = 4.533911305855124\n","Iteration 478 loss = 4.120677047363313\n","Iteration 479 loss = 4.939323119179929\n","Iteration 480 loss = 3.660528228716604\n","Iteration 481 loss = 3.7357770052529853\n","Iteration 482 loss = 3.0926929821062856\n","Iteration 483 loss = 4.609186465773331\n","Iteration 484 loss = 4.199843702528455\n","Iteration 485 loss = 4.605639352566653\n","Iteration 486 loss = 4.863279878813055\n","Iteration 487 loss = 4.307749303822952\n","Iteration 488 loss = 5.688285278201886\n","Iteration 489 loss = 4.70381326426851\n","Iteration 490 loss = 3.401895610239603\n","Iteration 491 loss = 3.897043828973132\n","Iteration 492 loss = 3.9096173453225407\n","Iteration 493 loss = 4.171999572590255\n","Iteration 494 loss = 4.178341422302001\n","Iteration 495 loss = 4.580514616435854\n","Iteration 496 loss = 3.974495927876371\n","Iteration 497 loss = 3.4876332955315386\n","Iteration 498 loss = 4.5316743389485925\n","Iteration 499 loss = 4.391665155194935\n","Iteration 500 loss = 5.9203933835572276\n","Iteration 501 loss = 4.7170545256203384\n","Iteration 502 loss = 3.773393246318874\n","Iteration 503 loss = 4.41342738565443\n","Iteration 504 loss = 4.893925807579355\n","Iteration 505 loss = 4.686664740647794\n","Iteration 506 loss = 3.1962226691948046\n","Iteration 507 loss = 4.172421967085651\n","Iteration 508 loss = 4.309716297053787\n","Iteration 509 loss = 4.7684076754799545\n","Iteration 510 loss = 4.596532458756634\n","Iteration 511 loss = 3.2928580240052248\n","Iteration 512 loss = 3.912431842868042\n","Iteration 513 loss = 5.480323292005683\n","Iteration 514 loss = 4.554597165865642\n","Iteration 515 loss = 4.460115340810978\n","Iteration 516 loss = 4.573443911274622\n","Iteration 517 loss = 4.6975944462727055\n","Iteration 518 loss = 3.573374461823545\n","Iteration 519 loss = 3.9174686891994512\n","Iteration 520 loss = 4.133661495042666\n","Iteration 521 loss = 4.78212792088238\n","Iteration 522 loss = 5.184796980476039\n","Iteration 523 loss = 3.7949199528930087\n","Iteration 524 loss = 4.430084268796389\n","Iteration 525 loss = 4.42682897121289\n","Iteration 526 loss = 4.463555996598509\n","Iteration 527 loss = 4.036201533886215\n","Iteration 528 loss = 4.090890709946631\n","Iteration 529 loss = 3.3550739795187656\n","Iteration 530 loss = 3.5133797205149757\n","Iteration 531 loss = 3.3783107568234527\n","Iteration 532 loss = 4.250218608413723\n","Iteration 533 loss = 5.172448538487181\n","Iteration 534 loss = 2.892443120092643\n","Iteration 535 loss = 3.715879339945392\n","Iteration 536 loss = 4.784210330865778\n","Iteration 537 loss = 5.155644072503911\n","Iteration 538 loss = 3.4156290471335007\n","Iteration 539 loss = 4.041523473136267\n","Iteration 540 loss = 5.1807994119624015\n","Iteration 541 loss = 4.5843755981050816\n","Iteration 542 loss = 3.4917689227917896\n","Iteration 543 loss = 4.688928711390123\n","Iteration 544 loss = 5.534426527869194\n","Iteration 545 loss = 4.538091823923196\n","Iteration 546 loss = 3.610521531880767\n","Iteration 547 loss = 4.24126558650167\n","Iteration 548 loss = 4.501709691421018\n","Iteration 549 loss = 3.7186952050173065\n","Iteration 550 loss = 5.1137619469630575\n","Iteration 551 loss = 4.702218181103215\n","Iteration 552 loss = 4.39147726761501\n","Iteration 553 loss = 4.956196282003443\n","Iteration 554 loss = 4.619496185201847\n","Iteration 555 loss = 3.058667026577349\n","Iteration 556 loss = 3.9389110016038216\n","Iteration 557 loss = 4.66040762215072\n","Iteration 558 loss = 4.314902439783026\n","Iteration 559 loss = 4.340361146061707\n","Iteration 560 loss = 4.290500264548609\n","Iteration 561 loss = 3.8326671595086346\n","Iteration 562 loss = 4.497797838312011\n","Iteration 563 loss = 4.452642158827254\n","Iteration 564 loss = 4.367411314474059\n","Iteration 565 loss = 4.430280620053659\n","Iteration 566 loss = 4.672988429931766\n","Iteration 567 loss = 4.522298898713396\n","Iteration 568 loss = 4.038112567259289\n","Iteration 569 loss = 2.8371693267591565\n","Iteration 570 loss = 3.224033845267115\n","Iteration 571 loss = 3.603965332535736\n","Iteration 572 loss = 5.242214385978292\n","Iteration 573 loss = 3.794399910883561\n","Iteration 574 loss = 3.288669206086641\n","Iteration 575 loss = 3.9610100917645843\n","Iteration 576 loss = 4.083034547333068\n","Iteration 577 loss = 3.8696326694653513\n","Iteration 578 loss = 3.8857383452933036\n","Iteration 579 loss = 4.140614787192871\n","Iteration 580 loss = 4.3364953925323695\n","Iteration 581 loss = 4.6394885554497165\n","Iteration 582 loss = 4.359537291330556\n","Iteration 583 loss = 4.217165711255555\n","Iteration 584 loss = 4.622219237700634\n","Iteration 585 loss = 4.7339787359497265\n","Iteration 586 loss = 4.560721586171111\n","Iteration 587 loss = 4.83804803511455\n","Iteration 588 loss = 4.513081084211316\n","Iteration 589 loss = 4.502066481318843\n","Iteration 590 loss = 4.533184337519359\n","Iteration 591 loss = 3.311826390283292\n","Iteration 592 loss = 3.291249155800856\n","Iteration 593 loss = 4.768517165591968\n","Iteration 594 loss = 5.052903990443222\n","Iteration 595 loss = 4.873285089559165\n","Iteration 596 loss = 3.7571324558234216\n","Iteration 597 loss = 4.043522409696671\n","Iteration 598 loss = 4.159183710233357\n","Iteration 599 loss = 4.683839563814317\n","Iteration 600 loss = 3.1727117584298337\n","Iteration 601 loss = 2.9292719077743046\n","Iteration 602 loss = 3.733244780237842\n","Iteration 603 loss = 3.835785499053213\n","Iteration 604 loss = 4.617077679413338\n","Iteration 605 loss = 4.65238368974821\n","Iteration 606 loss = 5.717187993963527\n","Iteration 607 loss = 5.656724254043272\n","Iteration 608 loss = 3.462998565038049\n","Iteration 609 loss = 5.203699645056547\n","Iteration 610 loss = 5.016609326904049\n","Iteration 611 loss = 3.8897039473371366\n","Iteration 612 loss = 4.088671863130423\n","Iteration 613 loss = 5.165687370969428\n","Iteration 614 loss = 4.342538198327325\n","Iteration 615 loss = 4.135066926001986\n","Iteration 616 loss = 3.8464196265850825\n","Iteration 617 loss = 3.3133393166190324\n","Iteration 618 loss = 4.020276263412549\n","Iteration 619 loss = 4.292880576437719\n","Iteration 620 loss = 4.158299567329061\n","Iteration 621 loss = 4.885355766033403\n","Iteration 622 loss = 5.479712532035276\n","Iteration 623 loss = 4.377843960607046\n","Iteration 624 loss = 5.319938967848164\n","Iteration 625 loss = 4.0301841547977215\n","Iteration 626 loss = 3.5418242818409404\n","Iteration 627 loss = 3.4756248087401365\n","Iteration 628 loss = 4.677575382621103\n","Iteration 629 loss = 2.707400095270946\n","Iteration 630 loss = 4.256073495080807\n","Iteration 631 loss = 3.2602217372511326\n","Iteration 632 loss = 4.168401824419756\n","Iteration 633 loss = 3.9450273371897455\n","Iteration 634 loss = 3.592096587651712\n","Iteration 635 loss = 4.112353339976144\n","Iteration 636 loss = 4.3438147046376905\n","Iteration 637 loss = 4.489556257653788\n","Iteration 638 loss = 5.8688931046491275\n","Iteration 639 loss = 3.411926550713618\n","Iteration 640 loss = 5.259352397200512\n","Iteration 641 loss = 4.836340699956482\n","Iteration 642 loss = 4.595935220981585\n","Iteration 643 loss = 4.044776180213542\n","Iteration 644 loss = 3.718967609122142\n","Iteration 645 loss = 4.691095336453378\n","Iteration 646 loss = 3.7950085006121834\n","Iteration 647 loss = 4.113801869243153\n","Iteration 648 loss = 4.182604871177931\n","Iteration 649 loss = 4.868199421694109\n","Iteration 650 loss = 3.9889592424650697\n","Iteration 651 loss = 4.456595636987293\n","Iteration 652 loss = 4.047406584277286\n","Iteration 653 loss = 4.299518158141485\n","Iteration 654 loss = 3.9828111083523954\n","Iteration 655 loss = 4.326378640074714\n","Iteration 656 loss = 4.162000824708796\n","Iteration 657 loss = 4.074591028585738\n","Iteration 658 loss = 3.2006316917769504\n","Iteration 659 loss = 3.5537396337409866\n","Iteration 660 loss = 3.515889435694077\n","Iteration 661 loss = 3.6649549722431836\n","Iteration 662 loss = 3.6131438029246845\n","Iteration 663 loss = 3.939843840760902\n","Iteration 664 loss = 3.279811311595066\n","Iteration 665 loss = 4.2816985919427255\n","Iteration 666 loss = 4.208994723361492\n","Iteration 667 loss = 4.428917723741834\n","Iteration 668 loss = 5.436932302121624\n","Iteration 669 loss = 4.5770547601738025\n","Iteration 670 loss = 4.973796986626005\n","Iteration 671 loss = 3.7882883252573376\n","Iteration 672 loss = 4.849777889420001\n","Iteration 673 loss = 3.9085760462110075\n","Iteration 674 loss = 4.2163863438374145\n","Iteration 675 loss = 5.050947986162539\n","Iteration 676 loss = 3.8637436908878886\n","Iteration 677 loss = 5.468755827134654\n","Iteration 678 loss = 5.2134542178433945\n","Iteration 679 loss = 3.7637996889223357\n","Iteration 680 loss = 4.505079448356081\n","Iteration 681 loss = 4.122454459542614\n","Iteration 682 loss = 4.3829635661493835\n","Iteration 683 loss = 5.267002894722052\n","Iteration 684 loss = 3.4001079192901433\n","Iteration 685 loss = 3.337340334848774\n","Iteration 686 loss = 4.6469950698303535\n","Iteration 687 loss = 4.814297264039231\n","Iteration 688 loss = 4.764788945386583\n","Iteration 689 loss = 3.9446959670165582\n","Iteration 690 loss = 2.95083504391012\n","Iteration 691 loss = 3.7053390302030613\n","Iteration 692 loss = 5.4149336611587655\n","Iteration 693 loss = 4.112721199105113\n","Iteration 694 loss = 4.237193892437299\n","Iteration 695 loss = 3.8906496550867686\n","Iteration 696 loss = 4.744660934278212\n","Iteration 697 loss = 3.270074541048793\n","Iteration 698 loss = 4.89028787201288\n","Iteration 699 loss = 4.38054076621344\n","Iteration 700 loss = 5.071461811490131\n","Iteration 701 loss = 4.245862416370899\n","Iteration 702 loss = 4.2448675538304945\n","Iteration 703 loss = 3.4428070414347514\n","Iteration 704 loss = 4.356266065978397\n","Iteration 705 loss = 3.491221141706822\n","Iteration 706 loss = 3.9454010843172638\n","Iteration 707 loss = 4.35257787307705\n","Iteration 708 loss = 4.86912533479024\n","Iteration 709 loss = 2.878339967728878\n","Iteration 710 loss = 4.05295769824637\n","Iteration 711 loss = 3.592376821561241\n","Iteration 712 loss = 4.254577758698431\n","Iteration 713 loss = 3.5035121929626554\n","Iteration 714 loss = 4.743510094041562\n","Iteration 715 loss = 4.647345068585565\n","Iteration 716 loss = 5.270005881366714\n","Iteration 717 loss = 4.416178729354472\n","Iteration 718 loss = 3.8999551146750893\n","Iteration 719 loss = 3.936842620357691\n","Iteration 720 loss = 5.719847253685776\n","Iteration 721 loss = 4.283176294949457\n","Iteration 722 loss = 4.315180122797902\n","Iteration 723 loss = 4.976428850934361\n","Iteration 724 loss = 4.800506210468831\n","Iteration 725 loss = 4.355741284538105\n","Iteration 726 loss = 3.7922257757587094\n","Iteration 727 loss = 3.6863370569961185\n","Iteration 728 loss = 4.701461014160936\n","Iteration 729 loss = 4.810461902580282\n","Iteration 730 loss = 4.468229270737425\n","Iteration 731 loss = 5.699074424084633\n","Iteration 732 loss = 3.69913284803319\n","Iteration 733 loss = 5.452379298906455\n","Iteration 734 loss = 3.8772803881834514\n","Iteration 735 loss = 3.7275190190611527\n","Iteration 736 loss = 4.919800827576908\n","Iteration 737 loss = 5.028326632994264\n","Iteration 738 loss = 3.892160004016532\n","Iteration 739 loss = 2.980223068973016\n","Iteration 740 loss = 3.812298628941758\n","Iteration 741 loss = 4.285773573842411\n","Iteration 742 loss = 4.55231373414388\n","Iteration 743 loss = 3.9204343763466873\n","Iteration 744 loss = 4.832085474275994\n","Iteration 745 loss = 4.845133271811654\n","Iteration 746 loss = 3.95079370495096\n","Iteration 747 loss = 4.650862037611742\n","Iteration 748 loss = 4.624310469385235\n","Iteration 749 loss = 5.3550416845820115\n","Iteration 750 loss = 4.564062638373204\n","Iteration 751 loss = 3.663212749138133\n","Iteration 752 loss = 5.105545308666099\n","Iteration 753 loss = 4.076107711028611\n","Iteration 754 loss = 4.662430983366331\n","Iteration 755 loss = 3.907211636857616\n","Iteration 756 loss = 3.8991396338355333\n","Iteration 757 loss = 4.4672837124503015\n","Iteration 758 loss = 4.358328816426737\n","Iteration 759 loss = 4.036337560473669\n","Iteration 760 loss = 5.171910516909524\n","Iteration 761 loss = 3.1565802772345903\n","Iteration 762 loss = 3.859109547510916\n","Iteration 763 loss = 4.834264700910138\n","Iteration 764 loss = 4.042334093085558\n","Iteration 765 loss = 5.434301728679652\n","Iteration 766 loss = 3.818761206447342\n","Iteration 767 loss = 4.025036344385858\n","Iteration 768 loss = 3.9111382974812954\n","Iteration 769 loss = 3.978089378719627\n","Iteration 770 loss = 3.7681960711266216\n","Iteration 771 loss = 4.27448950255827\n","Iteration 772 loss = 3.795091130847288\n","New model found! New loss: 2.5988683148685006\n","Iteration 774 loss = 4.751439947184122\n","Iteration 775 loss = 4.869521424019901\n","Iteration 776 loss = 3.707907799773047\n","Iteration 777 loss = 4.366056696052296\n","Iteration 778 loss = 4.648156038543779\n","Iteration 779 loss = 3.158406143637604\n","Iteration 780 loss = 4.587765621119673\n","Iteration 781 loss = 3.458303725030624\n","Iteration 782 loss = 4.038118096611665\n","Iteration 783 loss = 5.061193895203729\n","Iteration 784 loss = 3.5694006322130702\n","Iteration 785 loss = 5.455992826557321\n","Iteration 786 loss = 5.367681827122799\n","Iteration 787 loss = 6.052083793675384\n","Iteration 788 loss = 3.8340076750797474\n","Iteration 789 loss = 4.64536985663018\n","Iteration 790 loss = 3.9496362303790282\n","Iteration 791 loss = 3.978161530107176\n","Iteration 792 loss = 4.523729191376721\n","Iteration 793 loss = 4.365463970300578\n","Iteration 794 loss = 4.534181168001075\n","Iteration 795 loss = 3.868463299093956\n","Iteration 796 loss = 4.003068954325029\n","Iteration 797 loss = 4.452955136603976\n","Iteration 798 loss = 3.6672055994441863\n","Iteration 799 loss = 4.516351774618682\n","Iteration 800 loss = 4.8243999354972384\n","Iteration 801 loss = 3.903685712339444\n","Iteration 802 loss = 4.43738848396457\n","Iteration 803 loss = 4.5679767584669015\n","Iteration 804 loss = 3.853633798103652\n","Iteration 805 loss = 4.769548379131617\n","Iteration 806 loss = 3.8633678717915396\n","Iteration 807 loss = 4.006714591645873\n","Iteration 808 loss = 4.271422626388578\n","Iteration 809 loss = 4.766280495980111\n","Iteration 810 loss = 3.7067592437287042\n","Iteration 811 loss = 4.196064075627803\n","Iteration 812 loss = 3.9758240115123598\n","Iteration 813 loss = 4.261039064821637\n","Iteration 814 loss = 4.59150481027401\n","Iteration 815 loss = 5.227304300072701\n","Iteration 816 loss = 5.449036102320689\n","Iteration 817 loss = 4.809420184720917\n","Iteration 818 loss = 4.202088928434386\n","Iteration 819 loss = 3.930339015151096\n","Iteration 820 loss = 4.911856671682967\n","Iteration 821 loss = 3.429259078649113\n","Iteration 822 loss = 4.897539508643548\n","Iteration 823 loss = 3.8556362510460014\n","Iteration 824 loss = 3.6334021149666604\n","Iteration 825 loss = 4.489962202800709\n","Iteration 826 loss = 5.108136165986565\n","Iteration 827 loss = 3.9558448212114006\n","Iteration 828 loss = 3.6331277050363\n","Iteration 829 loss = 4.583523583997733\n","Iteration 830 loss = 3.590000343816938\n","Iteration 831 loss = 4.549691671479764\n","Iteration 832 loss = 4.458719667091247\n","Iteration 833 loss = 3.630567487430172\n","Iteration 834 loss = 3.47529567290953\n","Iteration 835 loss = 5.412553783446728\n","Iteration 836 loss = 3.9894275114494557\n","Iteration 837 loss = 3.9229938128533974\n","Iteration 838 loss = 3.627246159306003\n","Iteration 839 loss = 4.237650177680659\n","Iteration 840 loss = 3.465575303044677\n","Iteration 841 loss = 3.6472517894077767\n","Iteration 842 loss = 3.4523845100935397\n","Iteration 843 loss = 5.064136958632506\n","Iteration 844 loss = 3.9593790276491982\n","Iteration 845 loss = 3.907724372156164\n","Iteration 846 loss = 4.5949713770919685\n","Iteration 847 loss = 4.520809718499355\n","Iteration 848 loss = 4.469051700246444\n","Iteration 849 loss = 4.947943912350287\n","Iteration 850 loss = 4.508303562358951\n","Iteration 851 loss = 4.628946433731701\n","Iteration 852 loss = 4.0686658510855676\n","Iteration 853 loss = 4.456494480353274\n","Iteration 854 loss = 3.6753450942126573\n","Iteration 855 loss = 4.052349222530596\n","Iteration 856 loss = 3.2900159667477014\n","Iteration 857 loss = 5.0009084699283655\n","Iteration 858 loss = 4.216950592828395\n","Iteration 859 loss = 4.781988777744542\n","Iteration 860 loss = 4.970540514187114\n","Iteration 861 loss = 4.124108409374694\n","Iteration 862 loss = 4.960510056379048\n","Iteration 863 loss = 4.223145320635625\n","Iteration 864 loss = 3.937110222911977\n","Iteration 865 loss = 3.833131715412709\n","Iteration 866 loss = 4.940482422281684\n","Iteration 867 loss = 3.888813204840706\n","Iteration 868 loss = 4.125066581374966\n","Iteration 869 loss = 4.7324104819490795\n","Iteration 870 loss = 4.343945794932089\n","Iteration 871 loss = 4.711901216830154\n","Iteration 872 loss = 3.756777745809422\n","Iteration 873 loss = 3.36336161712772\n","Iteration 874 loss = 5.310279640566329\n","Iteration 875 loss = 3.9640976701367903\n","Iteration 876 loss = 4.29407568300548\n","Iteration 877 loss = 3.8033361889184523\n","Iteration 878 loss = 3.851590133070783\n","Iteration 879 loss = 4.256690088330266\n","Iteration 880 loss = 4.414440855319393\n","Iteration 881 loss = 4.44735056888063\n","Iteration 882 loss = 4.650082127756522\n","Iteration 883 loss = 3.9389014331511882\n","Iteration 884 loss = 4.6479352776372025\n","Iteration 885 loss = 4.731092959015249\n","Iteration 886 loss = 3.739147989763589\n","Iteration 887 loss = 5.027896564230027\n","Iteration 888 loss = 4.58315905306354\n","Iteration 889 loss = 3.9073920176095727\n","Iteration 890 loss = 4.605771139410575\n","Iteration 891 loss = 4.463842382096056\n","Iteration 892 loss = 3.430570333789384\n","Iteration 893 loss = 3.729345352738434\n","Iteration 894 loss = 3.5462706245668962\n","Iteration 895 loss = 4.130576936729354\n","Iteration 896 loss = 5.005608561153318\n","Iteration 897 loss = 3.901951678035234\n","Iteration 898 loss = 4.911610190900687\n","Iteration 899 loss = 4.211523602760884\n","Iteration 900 loss = 3.115709009416565\n","Iteration 901 loss = 4.391733302396698\n","Iteration 902 loss = 3.9890008746523207\n","Iteration 903 loss = 5.393120245936538\n","Iteration 904 loss = 5.038120227880671\n","Iteration 905 loss = 3.955127899085544\n","Iteration 906 loss = 4.301919132672373\n","Iteration 907 loss = 4.434491246518658\n","Iteration 908 loss = 4.028863329036549\n","Iteration 909 loss = 4.623605822688147\n","Iteration 910 loss = 3.296159878633009\n","Iteration 911 loss = 4.4853829096648745\n","Iteration 912 loss = 3.8186123709676\n","Iteration 913 loss = 4.214832861866546\n","Iteration 914 loss = 4.465642995836994\n","Iteration 915 loss = 4.073675758656842\n","Iteration 916 loss = 3.9682849611260798\n","Iteration 917 loss = 4.83973653291587\n","Iteration 918 loss = 4.6001205265776175\n","Iteration 919 loss = 4.305463351390811\n","Iteration 920 loss = 3.87394387016462\n","Iteration 921 loss = 4.791004636328974\n","Iteration 922 loss = 4.868606493939361\n","Iteration 923 loss = 5.1937159344246595\n","Iteration 924 loss = 4.280888066653142\n","Iteration 925 loss = 3.7716976571406753\n","Iteration 926 loss = 4.715706288700813\n","Iteration 927 loss = 4.46949606950566\n","Iteration 928 loss = 5.238162329437309\n","Iteration 929 loss = 4.370444606947965\n","Iteration 930 loss = 6.0732630393296665\n","Iteration 931 loss = 3.418785097582254\n","Iteration 932 loss = 4.357527456251484\n","Iteration 933 loss = 4.890456528037205\n","Iteration 934 loss = 3.8780520355568084\n","Iteration 935 loss = 4.5229910866899985\n","Iteration 936 loss = 4.046589718239037\n","Iteration 937 loss = 4.145641429858378\n","Iteration 938 loss = 4.423871218752924\n","Iteration 939 loss = 3.7658827742812515\n","Iteration 940 loss = 3.58733274359008\n","Iteration 941 loss = 4.860768814338937\n","Iteration 942 loss = 4.308793682405601\n","Iteration 943 loss = 2.8823986091491465\n","Iteration 944 loss = 5.1972123165210435\n","Iteration 945 loss = 3.839594268060259\n","Iteration 946 loss = 4.146051628429617\n","Iteration 947 loss = 4.202569826504439\n","Iteration 948 loss = 3.6737345032444404\n","Iteration 949 loss = 4.478473653073376\n","Iteration 950 loss = 4.716676462518222\n","Iteration 951 loss = 4.724821342181707\n","Iteration 952 loss = 3.6082319793582007\n","Iteration 953 loss = 3.9084256418069576\n","Iteration 954 loss = 3.826448260821958\n","Iteration 955 loss = 4.192965855937784\n","Iteration 956 loss = 4.571632777384008\n","Iteration 957 loss = 4.27585972758946\n","Iteration 958 loss = 4.539083238273673\n","Iteration 959 loss = 4.908350801719109\n","Iteration 960 loss = 5.404417289840763\n","Iteration 961 loss = 4.24251788484789\n","Iteration 962 loss = 3.783488754056117\n","Iteration 963 loss = 4.885301120008504\n","Iteration 964 loss = 4.037443449762282\n","Iteration 965 loss = 4.862019978669112\n","Iteration 966 loss = 4.2751589058463075\n","Iteration 967 loss = 4.8138481238445205\n","Iteration 968 loss = 4.374293626115341\n","Iteration 969 loss = 4.880532047699622\n","Iteration 970 loss = 4.575931437090873\n","Iteration 971 loss = 3.531402309288131\n","Iteration 972 loss = 4.085545941516286\n","Iteration 973 loss = 4.611368821475797\n","Iteration 974 loss = 4.590585678310156\n","Iteration 975 loss = 4.942903928030453\n","Iteration 976 loss = 3.6325105199720946\n","Iteration 977 loss = 4.153607945310996\n","Iteration 978 loss = 4.877835243881574\n","Iteration 979 loss = 5.035694392122809\n","Iteration 980 loss = 5.006867523150749\n","Iteration 981 loss = 4.121254245899643\n","Iteration 982 loss = 4.637406713187102\n","Iteration 983 loss = 4.4553519214358746\n","Iteration 984 loss = 4.8346956274465995\n","Iteration 985 loss = 4.649152795676964\n","Iteration 986 loss = 3.990916680391639\n","Iteration 987 loss = 4.053269015061912\n","Iteration 988 loss = 4.058815741622114\n","Iteration 989 loss = 4.400392066780591\n","Iteration 990 loss = 5.01660731767698\n","Iteration 991 loss = 4.047126580050415\n","Iteration 992 loss = 3.93479433628537\n","Iteration 993 loss = 4.470903822602072\n","Iteration 994 loss = 3.4305659054632693\n","Iteration 995 loss = 3.57761751954233\n","Iteration 996 loss = 4.6513261375969535\n","Iteration 997 loss = 3.6638989872223227\n","Iteration 998 loss = 3.4332086206791352\n","Iteration 999 loss = 3.732817478464388\n"]}],"source":["MAX_ITERATIONS = 1000\n","MIN_LOSS = np.sum(loss(predictions, train_labels))\n","for i in range(MAX_ITERATIONS):\n","    new_model = (np.random.rand(784,10) - 0.5) * 2 # Random values in the range [-1, 1)    \n","    new_predictions = sigmoid(train_vals.T @ new_model)\n","    new_loss = np.sum(loss(new_predictions, train_labels))\n","    if new_loss < MIN_LOSS:\n","        MIN_LOSS = new_loss\n","        new_model = model\n","        print(f\"New model found! New loss: {MIN_LOSS}\")\n","    else:\n","        print(f\"Iteration {i} loss = {new_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(784,)\n","(784, 10)\n","[0.7233194  0.98752347 0.06930624 0.03754042 0.9905272  0.01467198\n"," 0.27637816 0.96771305 0.13265511 0.03477478]\n","7.0\n"]}],"source":["print(test_vals.T[0].shape)\n","print(model.shape)\n","print(sigmoid(test_vals.T[0] @ model))\n","print(test_labels[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Network:\n","\n","    def __init__(self):\n","        self.input_layer_num = 784\n","        self.output_layer_num = 10\n","        self.hidden_layer_nums = [10, 10]"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":27352,"sourceId":34877,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
